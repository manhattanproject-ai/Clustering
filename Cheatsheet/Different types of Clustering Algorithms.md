# Different types of Clustering Algorithms

Clustering algorithms are a set of unsupervised learning techniques used to group a set of objects in such a way that objects in the same group (a cluster) are more similar to each other than to those in other groups.

<img width="1152" height="772" alt="image" src="https://github.com/user-attachments/assets/9958fadd-58e9-40e3-a7cf-f2400f63b3c3" />

## 1. Partition-Based Clustering ðŸš€
These algorithms divide the data points into K non-overlapping subsets (clusters), where K is specified beforehand. The goal is to minimize the intra-cluster distance.

* K-means Clustering: Assigns each data point to the nearest cluster centroid.
    * K-means++: An intelligent initialization method for K-means that selects initial cluster centers with greater separation.
    * K-mode: Variation of K-means used for clustering categorical data.
    * K-prototype: Combines K-means and K-mode to handle mixed (numerical and categorical) data.
    * Fuzzy C-means: Allows a data point to belong to multiple clusters with varying degrees of membership (fuzziness).
    * MiniBatch K-means: Uses a small, random subset of the data (mini-batch) to update the centroids, which speeds up convergence for large datasets.

## 2. Hierarchical Clustering ðŸš€
These methods build a hierarchy of clusters, either by starting with individual points and merging them (bottom-up) or starting with one large cluster and splitting it (top-down).

* Agglomerative Hierarchical Clustering (Bottom-Up): Starts with each data point as a single cluster and merges the closest pairs of clusters until only one cluster remains.
   * Linkage Methods (Single, Complete, Ward, Average): These define how the "distance" between two clusters is measured to determine which ones to merge.
* Divisive Hierarchical Clustering (Top-Down): Starts with all points in one cluster and recursively splits the most appropriate cluster until every point is a cluster.
* BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies): Designed for very large datasets; it summarizes the data using a Clustering Feature Tree and then applies a clustering method.

## 3. Density-Based Clustering ðŸš€
These algorithms define clusters as areas of high density separated by areas of low density. They are effective at finding clusters of arbitrary shapes and identifying noise/outliers.

* DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Groups points that are closely packed together, marking points that lie alone in low-density regions as outliers.
* HDBSCAN (Hierarchical DBSCAN): A hierarchical version of DBSCAN that transforms the density-based clusters into a hierarchy and then uses a stability measure to extract the most likely clusters.
* Mean Shift Clustering: An algorithm that shifts data points iteratively towards the mode (peak) of the local density until convergence.
* OPTICS (Ordering Points To Identify Clustering Structure): Creates an ordering of the data points representing its density-based clustering structure, allowing for parameter-free analysis.

## 4. Model-Based Clustering ðŸš€
These methods assume that the data was generated by an underlying probabilistic model.

* Gaussian Mixture Models (GMM): Assumes that the data points are generated from a mixture of several Gaussian distributions with unknown parameters. The goal is to estimate these parameters (mean, variance, and weight) for each distribution.

## 5. Graph-Based Models ðŸš€
These methods represent the data as a graph where data points are nodes, and similarities are edges. Clustering is then treated as a graph partitioning problem.

* Spectral Clustering: Uses the eigenvalues (spectrum) of the similarity matrix (derived from the data's graph representation) to perform dimensionality reduction before clustering in a lower-dimensional space.
